{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c2253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunLogRegr():\n",
    "    def __init__(self, X_train: np.ndarray, X_test: np.ndarray, y_train: pd.DataFrame, y_test: pd.DataFrame) -> None:\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    def build_logregr(self) -> None:\n",
    "        self.lr = LogisticRegression(max_iter=200, class_weight='balanced',\n",
    "                                     solver='liblinear', penalty='l1')\n",
    "        self.lr.fit(self.X_train, self.y_train['class'])\n",
    "    \n",
    "    def make_prediction(self) -> None:\n",
    "        '''\n",
    "        METHOD: make_prediction = now make a prediction on the test data with the optimal random forest model\n",
    "        '''\n",
    "        self.y_pred = self.lr.predict(self.X_test)\n",
    "    \n",
    "    def calc_scores(self) -> None:\n",
    "        '''\n",
    "        METHOD: calc_scores = calculate the accuracy, precision, recall, and F1-score\n",
    "        '''\n",
    "        self.accuracy = accuracy_score(self.y_test['class'], self.y_pred)\n",
    "        self.precision = precision_score(self.y_test['class'], self.y_pred)\n",
    "        self.recall = recall_score(self.y_test['class'], self.y_pred)\n",
    "        self.f1 = 2*self.precision*self.recall/(self.precision + self.recall)\n",
    "        \n",
    "        print(\"Accuracy =\", self.accuracy)\n",
    "        print(\"Precision =\", self.precision)\n",
    "        print(\"Recall =\", self.recall)\n",
    "        print(\"F1 =\", self.f1, '\\n')\n",
    "    \n",
    "    def print_confusion_matrix(self) -> None:\n",
    "        '''\n",
    "        METHOD: print_confusion_matrix = construct the confusion matrix on the test prediction\n",
    "        '''\n",
    "        cm = confusion_matrix(self.y_test['class'], self.y_pred)\n",
    "        ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
    "    \n",
    "    def run(self) -> None:\n",
    "        '''\n",
    "        METHOD: run = run all the methods in order\n",
    "        '''\n",
    "        self.build_logregr()\n",
    "        self.make_prediction()\n",
    "        self.calc_scores()\n",
    "        self.print_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561cfcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_metrics(y_test: pd.DataFrame) -> [float, float, float, float]:\n",
    "    '''\n",
    "    FUNCTION: control_metrics = calculate the metrics assuming we predict only 1's, this acts as a control\n",
    "          IN: y_test = the dataframe from the input data\n",
    "    '''\n",
    "    num_HC = len([val for val in y_test['class'] if val == 0])\n",
    "    num_PD = len([val for val in y_test['class'] if val == 1])\n",
    "\n",
    "    # assume predict only 1's, these values should be beaten by the random forest model's average metrics\n",
    "    control_accuracy = (num_PD + 0)/(num_PD + num_HC + 0 + 0)  # (TP + TN)/(TP + FP + TN + FN)\n",
    "    control_precision = num_PD/(num_PD + num_HC)  # TP/(TP + FP)\n",
    "    control_recall = num_PD/(num_PD + 0)  # TP/(TP + FN)\n",
    "    control_f1 = 2*control_precision*control_recall/(control_precision + control_recall)\n",
    "\n",
    "    print(\"Control Accuracy =\", control_accuracy)\n",
    "    print(\"Control Precision =\", control_precision)\n",
    "    print(\"Control Recall =\", control_recall)\n",
    "    print(\"Control F1 =\", control_f1)\n",
    "    \n",
    "    return control_accuracy, control_precision, control_recall, control_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439755cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "front_dir = '../data/split/kfold'\n",
    "\n",
    "def load_and_run(Kfolds: int) -> None:\n",
    "    control_accuracy = 0\n",
    "    control_precision = 0\n",
    "    control_recall = 0\n",
    "    control_f1 = 0\n",
    "\n",
    "    pred_accuracy = 0\n",
    "    pred_precision = 0\n",
    "    pred_recall = 0\n",
    "    pred_f1 = 0\n",
    "    \n",
    "    for kk in range(Kfolds):\n",
    "        X_train = np.loadtxt(front_dir + f'/X_train-{str(kk)}_kfold{str(Kfolds)}.csv', delimiter=',')\n",
    "        X_test = np.loadtxt(front_dir + f'/X_test-{str(kk)}_kfold{str(Kfolds)}.csv', delimiter=',')\n",
    "        y_train = pd.read_csv(front_dir + f'/y_train-{str(kk)}_kfold{str(Kfolds)}.csv', index_col=0)\n",
    "        y_test = pd.read_csv(front_dir + f'/y_test-{str(kk)}_kfold{str(Kfolds)}.csv', index_col=0)\n",
    "\n",
    "    #     print('---- X_train ----')\n",
    "    #     print(X_train, '\\n')\n",
    "    #     print('---- X_test ----')\n",
    "    #     print(X_test, '\\n')\n",
    "    #     print('---- y_train ----')\n",
    "    #     display(y_train)\n",
    "    #     print('---- y_test ----')\n",
    "    #     display(y_test)\n",
    "        \n",
    "        print(f\"- - FOLD {kk} - -\\n\")\n",
    "        controls = control_metrics(y_test)\n",
    "        control_accuracy += controls[0]\n",
    "        control_precision += controls[1]\n",
    "        control_recall += controls[2]\n",
    "        control_f1 += controls[3]\n",
    "        print('')\n",
    "        rlr = RunLogRegr(X_train, X_test, y_train, y_test)\n",
    "        rlr.run()\n",
    "        pred_accuracy += rlr.accuracy\n",
    "        pred_precision += rlr.precision\n",
    "        pred_recall += rlr.recall\n",
    "        pred_f1 += rlr.f1\n",
    "    \n",
    "    print(\"-------- Averaged metrics accross the folds --------\")\n",
    "    print(\"Control Accuracy =\", control_accuracy/Kfolds)\n",
    "    print(\"Control Precision =\", control_precision/Kfolds)\n",
    "    print(\"Control Recall =\", control_recall/Kfolds)\n",
    "    print(\"Control F1 =\", control_f1/Kfolds)\n",
    "    print('')\n",
    "    print(\"Predicted Accuracy =\", pred_accuracy/Kfolds)\n",
    "    print(\"Predicted Precision =\", pred_precision/Kfolds)\n",
    "    print(\"Predicted Recall =\", pred_recall/Kfolds)\n",
    "    print(\"Predicted F1 =\", pred_f1/Kfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a283a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# saga with elasticnet will not converge still\n",
    "# l2 norm performs worse than l1 norm, meaning feature selection is an issue\n",
    "\n",
    "load_and_run(Kfolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32390bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F- I-- N---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
